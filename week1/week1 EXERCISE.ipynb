{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Api key is good\n"
     ]
    }
   ],
   "source": [
    "# set up environment\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    print(\"No api key found!\")\n",
    "elif not openai_api_key.startswith('sk-proj-'):\n",
    "    print(\"Something isn't right with the api key\")\n",
    "else:\n",
    "    print(\"Api key is good\")\n",
    "\n",
    "\n",
    "gpt = OpenAI()\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Let's break down the code snippet you provided:\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "### Components of the Code:\n",
       "\n",
       "1. **`books`**: This likely refers to a collection (like a list) of dictionaries, where each dictionary represents a book with various attributes, one of which is `\"author\"`.\n",
       "\n",
       "2. **Comprehension**: The part `{book.get(\"author\") for book in books if book.get(\"author\")}` is a set comprehension. It constructs a set of authors from the `books` collection:\n",
       "   - `book.get(\"author\")`: This attempts to retrieve the value associated with the key `\"author\"` from each `book` dictionary.\n",
       "   - `for book in books`: This iterates through each `book` in the `books` collection.\n",
       "   - `if book.get(\"author\")`: This conditional checks that the value retrieved is truthy (i.e., the author exists and is not `None` or an empty string).\n",
       "\n",
       "3. **Set**: The use of curly braces `{}` indicates that the result is a set. In Python, a set automatically eliminates duplicate values, so if multiple books have the same author, that author will only appear once in the resulting set. \n",
       "\n",
       "4. **`yield from`**: This is used in a generator function to yield all values from an iterable (in this case, the set of authors). Instead of yielding items one at a time in a loop, `yield from` simplifies the code by yielding all items from the given iterable directly.\n",
       "\n",
       "### What the Code Does:\n",
       "\n",
       "- The code creates a set of unique authors from the `books` collection by extracting the `\"author\"` values from each book dictionary, while ignoring any books that do not have an `\"author\"` defined (i.e., `None` or empty).\n",
       "- The `yield from` statement means that rather than returning this set of authors at once, the code will yield each author one by one, making it suitable for use in a generator function. This allows the surrounding code to handle these authors one at a time.\n",
       "\n",
       "### Why Use This Code:\n",
       "\n",
       "- **Generality**: By using a generator, you can work with large datasets (like many books) without storing all authors in memory at once.\n",
       "- **Efficiency**: The set comprehension automatically filters out duplicates, ensuring that each author is only yielded once.\n",
       "- **Clean Code**: The use of `yield from` with the set comprehension makes the code concise and easy to understand, effectively separating the logic of constructing the author set from the logic of yielding each author.\n",
       "\n",
       "In summary, this code snippet efficiently retrieves and yields unique authors from a list of book dictionaries in a clean and memory-efficient manner."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "response = gpt.chat.completions.create(model=MODEL_GPT, \n",
    "                                       messages=[{\"role\": \"user\", \"content\": question}],\n",
    "                                       stream=True)\n",
    "result = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in response:\n",
    "    result += chunk.choices[0].delta.content or ''\n",
    "    update_display(Markdown(result), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "\n",
    "response = ollama.chat.completions.create(model=MODEL_LLAMA,\n",
    "                                          messages=[{\"role\":\"user\", \"content\": question}],\n",
    "                                          stream=True)\n",
    "result = \"\"\n",
    "display_handle = display(Markdown(result), display_id=True)\n",
    "for chunk in response:\n",
    "    result += chunk.choices[0].delta.content or ''\n",
    "    update_display(Markdown(result), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ae3d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
